{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52824b89-532a-4e54-87e9-1410813cd39e",
   "metadata": {},
   "source": [
    "# LangChain: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071b906",
   "metadata": {},
   "source": [
    "1️⃣ Map Reduce\n",
    "\n",
    "This method follows a two-step process:\n",
    "\n",
    "Map Step: The LLM processes each chunk of retrieved text independently and generates partial answers.\n",
    "Reduce Step: The outputs from all chunks are combined to form a final, coherent answer. The reduction step can involve summarization, majority voting, or another aggregation method.\n",
    "\n",
    "✅ Advantages:\n",
    "\n",
    "Efficient when processing large datasets.\n",
    "Each chunk is processed separately, making it scalable.\n",
    "\n",
    "❌ Disadvantages:\n",
    "\n",
    "Lacks interdependence between chunks in the first step, which may lead to missing cross-references.\n",
    "\n",
    "2️⃣ Refine\n",
    "\n",
    "This approach is more sequential and iterative:\n",
    "\n",
    "The LLM processes the first retrieved document and generates an initial answer.\n",
    "Each subsequent document modifies or enhances the previous answer.\n",
    "The final answer is generated after all chunks have been processed in order.\n",
    "\n",
    "✅ Advantages:\n",
    "\n",
    "Can refine and build a more comprehensive response by integrating new information step by step.\n",
    "Helps in handling multi-document reasoning.\n",
    "\n",
    "❌ Disadvantages:\n",
    "\n",
    "Can be computationally expensive as each step depends on the previous one.\n",
    "Errors in early stages may propagate through the refining process.\n",
    "\n",
    "3️⃣ Map Rerank\n",
    "\n",
    "This method also follows a two-step process, but with a ranking mechanism:\n",
    "\n",
    "Map Step: The LLM generates an answer for each chunk independently (like in Map Reduce).\n",
    "Rerank Step: Instead of summarizing, the LLM assigns a relevance score to each generated answer and selects the best one.\n",
    "\n",
    "✅ Advantages:\n",
    "\n",
    "Ensures that the most relevant and high-quality answer is chosen.\n",
    "Works well when retrieving multiple potential answers.\n",
    "\n",
    "❌ Disadvantages:\n",
    "\n",
    "May discard useful information if the best-scored answer isn't comprehensive.\n",
    "Requires a robust ranking mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc533037-0b8c-4995-96a3-45b35fa13c18",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import FAISS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7249846e",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bfaba30",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b5ab657",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Create FAISS vectorstore from documents\n",
    "faiss_index = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e200726",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Create a retriever from the FAISS index\n",
    "retriever = faiss_index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34562d81",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Define LLM model\n",
    "llm = ChatOpenAI(temperature=0.0, model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfd0cc37",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Query for shirts with sun protection\n",
    "query = \"Please list all your shirts with sun protection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae21f1ff",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Retrieve relevant documents from FAISS index\n",
    "retrieved_docs = faiss_index.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "631396c6",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Combine retrieved document content\n",
    "qdocs = \"\\n\".join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c2164b5",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Generate response using LLM\n",
    "response = llm.call_as_llm(f\"{qdocs} Question: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a977f44",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Create RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e875693a",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run query using RetrievalQA\n",
    "qa_response = qa_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "779bec75",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Create FAISS index using VectorstoreIndexCreator\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "699aaaf9",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Query the FAISS index\n",
    "final_response = index.query(query, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d00d346",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Men's Plaid Tropic Shirt, Short-Sleeve\n",
       "2. Men's Tropical Plaid Short-Sleeve Shirt\n",
       "3. Men's TropicVibe Shirt, Short-Sleeve\n",
       "4. Girls' Ocean Breeze Long-Sleeve Stripe Shirt"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the final markdown formatted response\n",
    "display(Markdown(final_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5144aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
